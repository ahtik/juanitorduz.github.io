<!DOCTYPE html>
<html lang="en-us">
  <head>
    <meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<meta name="generator" content="Hugo 0.52" />


<title> - A Hugo website</title>
<meta property="og:title" content=" - A Hugo website">



  








<link href='//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/styles/github.min.css' rel='stylesheet' type='text/css' />



<link rel="stylesheet" href="/css/fonts.css" media="all">
<link rel="stylesheet" href="/css/main.css" media="all">



  </head>
  <body>
    <div class="wrapper">
      <header class="header">
        <nav class="nav">
  <a href="/" class="nav-logo">
    <img src="/images/logo.png"
         width="50"
         height="50"
         alt="Logo">
  </a>

  <ul class="nav-links">
    
    <li><a href="/about/">About</a></li>
    
    <li><a href="https://github.com/rstudio/blogdown">GitHub</a></li>
    
    <li><a href="https://twitter.com/rstudio">Twitter</a></li>
    
  </ul>
</nav>

      </header>


<main class="content" role="main">

  <article class="article">
    

    <h1 class="article-title"></h1>

    

    <div class="article-content">
      

<h1 id="introduction-to-bayesian-modeling-with-pymc3">Introduction to Bayesian modeling with PyMC3</h1>

<p>This post is devoted to give an introduction to Bayesian modeling using <a href="https://pymc-devs.github.io/pymc3/notebooks/getting_started.html">PyMC3</a>, an open source probabilistic programming framework written in Python. Part of this material was presented in the Python Users Berlin (PUB) meet up.</p>

<p><img src="images/PyMC3_banner.svg" alt="html" style="width: 400px;"/></p>

<p>Why PyMC3? As described in the documentation:</p>

<ul>
<li><p>PyMC3’s user-facing features are written in pure Python, it leverages <a href="http://deeplearning.net/software/theano/">Theano</a> to transparently transcode models to C and compile them to machine code, thereby boosting performance.</p></li>

<li><p>Theano is a library that allows expressions to be defined using generalized vector data structures called tensors, which are tightly integrated with the popular <a href="http://www.numpy.org/">NumPy</a> ndarray data structure.</p></li>
</ul>

<p>In addition, from a practical point of view, PyMC3 syntax is very transpartent from the mathematical point of view.</p>

<p>This post is not aimed to give a full treatment of the <del>mathematical details</del>, as there are many good (complete and detailed) references around these topics. Also, we are not going to dive deep into PyMC3 as all the details can be found in the documentation. Instead, we are interested in giving an overview of the basic mathematical consepts combinded with examples (writen in Python code) which should make clear why <a href="https://en.wikipedia.org/wiki/Monte_Carlo_method">Monte Carlo</a> simulations are useful in Bayesian modeling.</p>

<h1 id="1-mathematical-background">1. Mathematical Background</h1>

<h1 id="1-1-bayes-theorem">1.1 Bayes Theorem</h1>

<h2 id="frequentist-vs-bayesian">Frequentist vs Bayesian</h2>

<p><em>The essential difference between frequentist inference and Bayesian inference is the same as the difference between the two interpretations of what a &ldquo;probability&rdquo; means</em>.</p>

<p><strong>Frequentist inference</strong> is a method of statistical inference in which conclusions from data is obtained by emphasizing the frequency or proportion of the data.</p>

<p><strong>Bayesian inference</strong> is a method of statistical inference in which Bayes&rsquo; theorem is used to update the probability for a hypothesis as more evidence or information becomes available.</p>

<h2 id="conditional-probability">Conditional Probability</h2>

<p>Let \(A\) and \(B\) be two events, then the <em>conditional probability</em> of \(A\) given \(B\) is defined as the ratio</p>

<p>\begin{equation}
P(A|B):=\frac{P(A\cap B)}{P(B)}
\end{equation}</p>

<p><em>Remark:</em> Formally we have a <a href="https://en.wikipedia.org/wiki/Probability_space">probability space</a> \((\Omega, \mathcal{F}, P)\), where \(\Omega\) is the sample space, \(\mathcal{F}\) is a \(\sigma\)-algebra on \(\Omega\) and \(P\) is a probability measure. The events \(A\), and \(B\) are elements of \(\mathcal{F}\) and we assume that \(P(B)\neq 0\).</p>

<p>Observe in particular</p>

<p>\begin{equation}
P(A|B)P(B)=P(A\cap B)=P(B\cap A) = P(B|A)P(A)
\end{equation}</p>

<h2 id="bayes-theorem">Bayes Theorem</h2>

<p>From the last formula we obtain the relation</p>

<p>\begin{equation}
P(A|B)=\frac{P(B|A)P(A)}{P(B)}
\end{equation}</p>

<p>which is known as <a href="https://en.wikipedia.org/wiki/Bayes%27_theorem">Bayes Theorem</a>.</p>

<p><strong>Example:</strong> Suppose you are in the U-Bahn and you see a person with long hair. You want to know the probablity that this person is a woman. Consider the events \(A=\) woman \(B=\) long hair. You want to compute \(P(A|B)\). Suppose that you estimate \(P(A)=0.5\), \(P(B)=0.4\) and \(P(B|A)=0.7\) (the probability that a woman has long hair). Then, given these prior estimated probabilities, Bayes theorem gives</p>

<p>\begin{equation}
P(A|B)=\frac{P(B|A)P(A)}{P(B)} = \frac{0.7\times 0.5}{0.4} = 0.875.
\end{equation}</p>

<h2 id="bayesian-approach-to-data-analysis">Bayesian Approach to Data Analysis</h2>

<p>Assume that you have a sample of observations \(y_1,&hellip;, y_n\) of a random variable \(Y\sim f(y|\theta)\), where \(\theta\) is a parameter for the distribution. Here we consider \(\theta\) as a random variable as well. Following Bayes Theorem (its continuous version) we can write.</p>

<p>\begin{equation}
f(\theta|y)=\frac{f(y|\theta)f(\theta)}{f(y)} =
\displaystyle{\frac{f(y|\theta)f(\theta)}{\int f(y|\theta)f(\theta)d\theta}}
\end{equation}</p>

<ul>
<li><p>The function \(f(y|\theta)\) is called the <em>likelihood</em>.</p></li>

<li><p>\(f(\theta)\) is the <em>prior</em> distribution of \(\theta\).</p></li>
</ul>

<p>Note that \(f(y)\) <em>does not</em> depend on \(\theta\) (just on the data), thus it can be considered as a &ldquo;normalizing constant&rdquo;. In addition, it is often the case that the integral above is not easy to compute. Nevertheless, it is enough to consider the relation:</p>

<p>\begin{equation}
f(\theta|y)  \propto \text{likelihood} \times \text{prior}.
\end{equation}</p>

<p>(Here \(\propto\) denotes the proportionality relation)</p>

<h2 id="example-poisson-data">Example: Poisson Data</h2>

<p>In order to give a better sense of the relation above we are going to study a concrete example. Consider a \(n\) samples of \(Y\sim Poiss(\lambda)\). Recall that the <a href="https://en.wikipedia.org/wiki/Poisson_distribution">Poisson distribution</a> is given by:</p>

<p>$$
\displaystyle{
f(y_i|\lambda)=\frac{e^{-\lambda}\lambda^{y_i}}{y_i!}
}
$$</p>

<p>where \(\lambda&gt;0\). It is easy to verify that \(E(Y)=\lambda\) and \(Var(Y)=\lambda\). Parallel to the formal discussion, we are going to implement a numerical simulation:</p>

<pre><code class="language-python">import numpy as np
import scipy.stats as ss

# We set a seed so that the results are reproducible.
np.random.seed(5)

# number of samples.
n = 100

# true parameter.
lam_true = 2

# sample array.
y = np.random.poisson(lam=lam_true, size=n)

y
</code></pre>

<pre><code>array([2, 4, 1, 0, 2, 2, 2, 2, 1, 1, 3, 2, 0, 1, 3, 3, 4, 2, 0, 0, 3, 6, 1,
       2, 1, 2, 5, 2, 3, 0, 1, 3, 1, 4, 1, 2, 4, 0, 6, 4, 1, 2, 2, 0, 1, 2,
       4, 4, 1, 3, 0, 3, 3, 2, 4, 2, 2, 1, 1, 2, 5, 2, 3, 0, 1, 1, 1, 3, 4,
       1, 3, 4, 2, 1, 2, 4, 2, 2, 1, 0, 2, 2, 3, 0, 3, 3, 4, 2, 2, 1, 2, 1,
       3, 0, 1, 0, 3, 3, 1, 2])
</code></pre>

<pre><code class="language-python"># mean of the sample.
y.mean()
</code></pre>

<pre><code>2.0600000000000001
</code></pre>

<pre><code class="language-python">import matplotlib.pyplot as plt
%matplotlib inline

# Histogram of the sample.
plt.hist(y, bins=15, normed=False)
plt.title('Histogram of the Simulated Data')
plt.show()
</code></pre>

<p><img src="intro_PyMC3_files/intro_PyMC3_8_0.png" alt="png" /></p>

<h3 id="prior-gamma-distribution">Prior: Gamma Distribution</h3>

<p>Let us consider a <a href="https://en.wikipedia.org/wiki/Gamma_distribution">gamma</a> prior distribution for the parameter \(\lambda \sim \Gamma(a,b)\). Recall that the density function for the gamma distribution is</p>

<p>\begin{equation}
f(\lambda)=\frac{b^a}{\Gamma(a)}\lambda^{a-1} e^{-b\lambda}
\end{equation}</p>

<p>where \(a&gt;0\) is the <em>shape</em> parameter and \(b&gt;0\) is the <em>rate parameter</em>.</p>

<p>The <em>expected value</em> and <em>variance</em> of the gamma distribution is</p>

<p>$$
E(\lambda)=\frac{a}{b}
\quad
\text{and}
\quad
Var(\lambda)=\frac{a}{b^2}
$$</p>

<pre><code class="language-python"># Parameters of the prior gamma distribution.
a = 3.5 # shape
b = 2 # rate = 1/scale

x = np.linspace(0,10)
plt.plot(x, ss.gamma.pdf(x,a=a,scale=1/b), 'r-')
plt.title('Gamma Density Function for a={} and b={}'.format(a,b))
plt.show()

# Define the prior distribution.
prior = lambda x: ss.gamma.pdf(x,a=a,scale=1/b)
</code></pre>

<p><img src="intro_PyMC3_files/intro_PyMC3_10_0.png" alt="png" /></p>

<h3 id="likelihood">Likelihood</h3>

<p>As the observations are independent the <a href="https://en.wikipedia.org/wiki/Likelihood_function">likelihood</a> function is</p>

<p>\begin{align}
f(y|\lambda)=&amp;\prod_{i=1}^{n} \frac{e^{-\lambda}\lambda^{y_i}}{y<em>i!}
=\frac{e^{-n\lambda}\lambda^{\sum</em>{i=1}^n y<em>i}}{\prod</em>{i=1}^{n}y_i!}
\end{align}</p>

<pre><code class="language-python">import scipy.special as sp

# Define the likelihood function.
def likelihood(lam,y):
    
    factorials = np.apply_along_axis(lambda x: sp.gamma(x+1),
                                     axis=0,
                                     arr=y)
    
    numerator = np.exp(-lam*y.size)*(lam**y.sum())
    
    denominator = np.multiply.reduce(factorials)
    
    return numerator/denominator  
</code></pre>

<h3 id="posterior-distribution-for-lambda-up-to-a-constant">Posterior distribution for \(\lambda\) up to a constant</h3>

<p>As we are just interested in the structure of the posterior distribution, up to a constant, we see</p>

<p>\begin{align}
f(\lambda|y)\propto &amp; \text{likelihood} \times \text{prior}<br />
\propto &amp; \quad f(y|\lambda)f(\lambda)<br />
\propto &amp; \quad e^{-n\lambda}\lambda^{\sum_{i=1}^n y<em>i} \lambda^{a-1} e^{-b\lambda}<br />
\propto &amp; \quad \lambda^{\left(\sum</em>{i=1}^n y_i+a\right)-1} e^{-(n+b)\lambda}<br />
\end{align}</p>

<pre><code class="language-python"># Define the posterior distribution.
# (up to a constant)
def posterior_up_to_constant(lam,y):
    return likelihood(lam,y)*prior(lam)

# Plot of the prior and (scaled) posterior distribution
# for the parameter lambda.
#
# We multiply the posterior distrubution function
# by the amplitude factor 2.5e74 to make it comparable
# with the prior gamma distribution.
plt.plot(x, 2.5e74*posterior_up_to_constant(x,y))
plt.plot(x, ss.gamma.pdf(x,a=a,scale=1/b), 'r-')
plt.show()
</code></pre>

<p><img src="intro_PyMC3_files/intro_PyMC3_14_0.png" alt="png" /></p>

<h3 id="true-posterior-distribution-for-lambda">True posterior distribution for \(\lambda\)</h3>

<p>In fact, as  \(f(\lambda|y) \propto: \lambda^{\left(\sum_{i=1}^n y_i+a\right)-1} e^{-(n+b)\lambda}\), one verifies that the posterior distribution is again a gamma</p>

<p>\begin{align}
f(\lambda|y) = \Gamma\left(\sum_{i=1}^n y_i+a, n+b\right)
\end{align}</p>

<p>This means that the gamma and Poisson distribution form a <a href="https://en.wikipedia.org/wiki/Conjugate_prior">conjugate pair</a>.</p>

<pre><code class="language-python">def posterior(lam,y):
    
    shape = a + y.sum()
    rate = b + y.size
    
    return ss.gamma.pdf(lam, shape, scale=1/rate)

plt.plot(x, posterior(x,y))
plt.plot(x, ss.gamma.pdf(x,a=a,scale=1/b), 'r-')
plt.title('Prior and Posterior Distributions')
plt.show()
</code></pre>

<p><img src="intro_PyMC3_files/intro_PyMC3_16_0.png" alt="png" /></p>

<p>We indeed see how the posterior distribution is concentrated around the true parameter \(\lambda=2\).</p>

<p>Note that the posterior mean is</p>

<p>\begin{align}
\frac{\sum_{i=1}^n y<em>i+a}{n+b} = \frac{b}{n+b}\frac{a}{b}+\frac{n}{n+b}\frac{\sum</em>{i=1}^n y_i}{n}
\end{align}</p>

<p>That is, it is a weighted average of the prior mean \(a/b\) and the sample average \(\bar{y}\). As \(n\) increases,</p>

<p>\begin{align}
\lim<em>{n\rightarrow +\infty}\frac{b}{n+b}\frac{a}{b}+\frac{n}{n+b}\frac{\sum</em>{i=1}^n y_i}{n} = \bar{y}.
\end{align}</p>

<pre><code class="language-python"># Posterior gamma parameters.
shape = a + y.sum()
rate = b + y.size

# Posterior mean.
shape/rate
</code></pre>

<pre><code>2.0539215686274508
</code></pre>

<h1 id="1-2-markov-chain-monte-carlo-mcmc-approach">1.2 Markov Chain Monte Carlo  (MCMC) Approach</h1>

<p>In the last example the posterior distribution was easy to identify. However, in paractice this is not usually the case and therefore, via Bayes Theorem, we would only know the posterior distribution up to a constant. This motivates the idea of using Monte Carlo simulation methods. How can we sample from a distribution that we do not know? The Metropolis–Hastings algorithm, explaned next, is one approach to tackle this problem.</p>

<h2 id="metropolis-hastings-algorithm">Metropolis–Hastings algorithm</h2>

<p>Let \(\phi\) be a function that is proportional to the desired probability distribution \(f\).</p>

<p><strong>Initialization:</strong></p>

<p>Pick \(x_{0}\) to be the first sample, and choose an arbitrary probability density</p>

<p>\begin{equation}
g(x<em>{n+1}| x</em>{n})
\end{equation}</p>

<p>that suggests a candidate for the next sample value \(x_{n+1}\). Assume \(g\) is symmetric.</p>

<p><strong>For each iteration:</strong></p>

<p>Generate a candidate \(x\) for the next sample by picking from the distribution \(g(x|x_n)\). Calculate the <em>acceptance ratio</em></p>

<p>\begin{equation}
\alpha := \frac{f(x)}{f(x_n)} = \frac{\phi(x)}{\phi(x_n)}
\end{equation}</p>

<p>If \(\alpha \geq 1\), automatically accept the candidate by setting</p>

<p>\begin{equation}
x_{n+1} = x.
\end{equation}</p>

<p>Otherwise, accept the candidate with probability \(\alpha \). If the candidate is rejected, set</p>

<p>\begin{equation}
x<em>{n+1} = x</em>{n}.
\end{equation}</p>

<p>Why does this algorithm solve the initial problem? The full explanation is beyond the scope of this post (some references are provided at the end). It relies in the in the following result.</p>

<h2 id="ergodic-theorem-for-markov-chains">Ergodic Theorem for Markov Chains</h2>

<p><strong>Theorem (Ergodic Theorem for Markov Chains)</strong> If \({x^{(1)} , x^{(2)} , &hellip;}\) is an <em>irreducible</em>, <em>aperiodic</em> and <em>recurrent</em> <a href="https://en.wikipedia.org/wiki/Markov_chain">Markov chain</a>, then there is a unique probability distribution \(\pi\) such that as \(N\longrightarrow\infty\),</p>

<ul>
<li>\(P(x^{(N)} ∈ A) \longrightarrow \pi(A)\).</li>
<li>\(\displaystyle{\frac{1}{N}\sum_{n=1}^{N} g(x^{(n)})) \longrightarrow    \int g(x)\pi(x) dx }\).</li>
</ul>

<p><em>Recall:</em></p>

<ul>
<li><p>A Markov chain is said to be <strong>irreducible</strong> if it is possible to get to any state from any state.</p></li>

<li><p>A state \(n\) has <strong>period</strong> \(k\) if any return to state \(n\) must occur in multiples of \(k\) time steps.</p></li>

<li><p>If \(k=1\), then the state is said to be <strong>aperiodic</strong>.</p></li>

<li><p>A state \(n\) is said to be <strong>transient</strong> if, given that we start in state \(n\), there is a non-zero probability that we will never return to \(i\).</p></li>

<li><p>A state \(n\) is <strong>recurrent</strong>  if it is not transient.</p></li>
</ul>

<h1 id="2-pymc3-syntax">2. PyMC3 Syntax</h1>

<p>Now we perform a MCMC simulation for the data described above. Note how easy is to write the model from the mathematical description.</p>

<pre><code class="language-python">import pymc3 as pm

model = pm.Model()

with model:
    
    # Define the prior of the parameter lambda.
    lam = pm.Gamma('lambda', alpha=a, beta=b)
    
    # Define the likelihood function.
    y_obs = pm.Poisson('y_obs', mu=lam, observed=y)
    
    # Consider 2000 draws and 3 chains.
    trace = pm.sample(draws=2000, njobs=3)
</code></pre>

<pre><code>Auto-assigning NUTS sampler...
Initializing NUTS using ADVI...
Average Loss = 175.21:   4%|▍         | 8203/200000 [00:01&lt;00:28, 6752.78it/s]
Convergence archived at 8500
Interrupted at 8,500 [4%]: Average Loss = 188.36
100%|██████████| 2500/2500 [00:04&lt;00:00, 562.12it/s]
</code></pre>

<p>If we do a trace plot we can see two results:</p>

<ul>
<li><p>We see the simulated posterior distribution for 3 independent Markov Chains (so that, when combined, avoid the dependence on the initial point). The 3 different chains correspond to the color blue, green and orange.</p></li>

<li><p>The sample value of lambda for each iteration.</p></li>
</ul>

<pre><code class="language-python">pm.traceplot(trace);
</code></pre>

<p><img src="intro_PyMC3_files/intro_PyMC3_26_0.png" alt="png" /></p>

<p>We can also see the mean and quantile information for the posterior distribution.</p>

<pre><code class="language-python">pm.plot_posterior(trace);
</code></pre>

<p><img src="intro_PyMC3_files/intro_PyMC3_28_0.png" alt="png" /></p>

<h1 id="3-bayesian-hierarchical-modeling-a-chocolate-cookies-example">3. Bayesian Hierarchical Modeling: A Chocolate Cookies Example.</h1>

<p><img src="images/monster.jpg" alt="html" style="width: 400px;"/></p>

<p>Now we are going to treat a more complicated example which illustrated a hierarchical mdel.
This case of study is taken from the (strongly recomended!) online course:</p>

<p><strong>Bayesian Statistics: Techniques and Models:</strong></p>

<p><a href="https://www.coursera.org/learn/mcmc-bayesian-statistics">https://www.coursera.org/learn/mcmc-bayesian-statistics</a></p>

<p>There, the MCMC simulations are done with <a href="http://mcmc-jags.sourceforge.net/">JAGS</a> in <a href="https://www.r-project.org/">R</a>. As a matter of fact, this course motivated me to explore an analogous tool for Python.</p>

<h2 id="3-1-the-data">3.1 The data</h2>

<p>Assume there is a big factory producing chocolate cookies around the world. The cookies follow a unique recipe, but you want to study the chocolate chips distribution for cookies produced in 5 different locations.</p>

<ul>
<li><p>On the one hand side you would assume that the distribution across the locations is similar, as they all come from a unique recipe. This is why you may not want to model each location separately.</p></li>

<li><p>On the other hand, in reality, as the locations are not exacly the same you might expect some differences between each location.  This is why you may not want to model all locations at once.</p></li>
</ul>

<p>To overcome these restrictions, a hierarchical can be a feasible approach.</p>

<pre><code class="language-python">import pandas as pd

# We begin reading the data into a pandas dataframe.
cookies = pd.read_csv('data/cookies.dat', sep = ' ')

cookies.head()
</code></pre>

<div>
<style>
    .dataframe thead tr:only-child th {
        text-align: right;
    }

    .dataframe thead th {
        text-align: left;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>chips</th>
      <th>location</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>12</td>
      <td>1</td>
    </tr>
    <tr>
      <th>1</th>
      <td>12</td>
      <td>1</td>
    </tr>
    <tr>
      <th>2</th>
      <td>6</td>
      <td>1</td>
    </tr>
    <tr>
      <th>3</th>
      <td>13</td>
      <td>1</td>
    </tr>
    <tr>
      <th>4</th>
      <td>12</td>
      <td>1</td>
    </tr>
  </tbody>
</table>
</div>

<pre><code class="language-python"># Let us verify the number of locations.
cookies.location.unique()
</code></pre>

<pre><code>array([1, 2, 3, 4, 5])
</code></pre>

<p>Let us start with some visualization of the data.</p>

<pre><code class="language-python"># Histogram distribution of chocolate chips
# for all cookies.
cookies.hist(column='chips', bins=10);
</code></pre>

<p><img src="intro_PyMC3_files/intro_PyMC3_34_0.png" alt="png" /></p>

<pre><code class="language-python"># Histogram distribution of chocolate chips
# for cookies in each location.
cookies.hist(column='chips', by='location', bins=10);
</code></pre>

<p><img src="intro_PyMC3_files/intro_PyMC3_35_0.png" alt="png" /></p>

<pre><code class="language-python"># Box plot for different locations.
cookies.boxplot(column='chips', by='location', figsize=(10,6));
</code></pre>

<p><img src="intro_PyMC3_files/intro_PyMC3_36_0.png" alt="png" /></p>

<h2 id="3-2-the-model-hierarchical-approach">3.2 The model: Hierarchical Approach</h2>

<ul>
<li>Hierarchical Model:</li>
</ul>

<p>We model the chocolate chip counts by a Poisson distribution with parameter \(\lambda\). Motivated by the example above, we choose a gamma prior.</p>

<p>\begin{align}
chips \sim  Poiss(\lambda)
\quad\quad\quad
\lambda \sim  \Gamma(a,b)
\end{align}</p>

<ul>
<li>Parametrization:</li>
</ul>

<p>We parametrize the shape and scale of the gamma prior with the mean \(\mu\) and variance \(\sigma^2\).</p>

<p>\begin{align}
a=\frac{\mu^2}{\sigma^2}
\quad\quad\quad
b=\frac{\mu}{\sigma^2}
\end{align}</p>

<ul>
<li>Prior Distributions:</li>
</ul>

<p>We further impose prior for these parameters</p>

<p>\begin{align}
\mu  \sim  \Gamma(2,<sup>1</sup>&frasl;<sub>5</sub>)
\quad\quad\quad
\sigma  \sim  Exp(1)
\end{align}</p>

<pre><code class="language-python">x = np.linspace(0,50)

fig = plt.figure(figsize=(15, 5))
plt.subplot(1, 2, 1)
plt.plot(x, ss.gamma.pdf(x,a=2,scale=5), 'r-')
plt.title('Prior Distribution for mu \n Gamma Density Function for a={} and b={}'.format(2,1/5))

plt.subplot(1, 2, 2)
x = np.linspace(0,10)
plt.plot(x, ss.expon.pdf(x,1), 'r-')
plt.title('Prior Distribution for sigma2 \n Exponential Density Function')
plt.xlim(1,10)


plt.show()
</code></pre>

<p><img src="intro_PyMC3_files/intro_PyMC3_38_0.png" alt="png" /></p>

<p>Let us write the model in PyMC3. Note how the syntax mimics the mathematical formulation.</p>

<pre><code class="language-python">model = pm.Model()

with model:
    
    # Prior distribution for mu.
    mu = pm.Gamma('mu', alpha=2.0, beta=1.0/5)
    
    # Prior distribution for sigma2.
    sigma = pm.Exponential('sigma', 1.0)
    
    # Parametrization for the shape parameter.
    alpha =  mu**2/sigma**2
    
    # Parametrization for the scale parameter.
    beta = mu/sigma**2
    
    # Prior distribution for lambda.
    lam = pm.Gamma('lam', alpha=alpha, beta=beta, 
                   shape=cookies.location.values.max())
    
    # Likelihood function for the data.
    chips = [pm.Poisson('chips_{}'.format(i),lam[i], 
            observed=cookies[cookies.location==i+1].chips.values) 
            for i in range(cookies.location.values.max())] 
    
    
    # Parameters of the simulation:
    # Number of iterations and independent chains.
    n_draws, n_chains = 1000, 3
    
    n_sim = n_draws*n_chains
    
    trace = pm.sample(draws=n_draws, njobs=n_chains)  
</code></pre>

<pre><code>Auto-assigning NUTS sampler...
Initializing NUTS using ADVI...
Average Loss = 413.34:   5%|▌         | 10199/200000 [00:03&lt;00:49, 3805.65it/s]
Convergence archived at 10300
Interrupted at 10,300 [5%]: Average Loss = 768.84
100%|██████████| 1500/1500 [00:18&lt;00:00, 79.46it/s] 
</code></pre>

<h2 id="3-3-diagnostics">3.3 Diagnostics</h2>

<p>Many <a href="https://pymc-devs.github.io/pymc3/api/diagnostics.html">diagnostic</a> options are described in the PyMC3 documentation.</p>

<pre><code class="language-python">pm.traceplot(trace);
</code></pre>

<p><img src="intro_PyMC3_files/intro_PyMC3_42_0.png" alt="png" /></p>

<p>From the traceplot we see that the chains have converged. We can also have a detailed summary of the posterior distribution for each parameter:</p>

<pre><code class="language-python">pm.summary(trace)
</code></pre>

<pre><code>mu:

  Mean             SD               MC Error         95% HPD interval
  -------------------------------------------------------------------

  9.152            1.001            0.020            [7.202, 11.155]

  Posterior quantiles:
  2.5            25             50             75             97.5
  |--------------|==============|==============|--------------|

  7.281          8.544          9.123          9.756          11.285


sigma:

  Mean             SD               MC Error         95% HPD interval
  -------------------------------------------------------------------

  2.125            0.718            0.021            [1.001, 3.550]

  Posterior quantiles:
  2.5            25             50             75             97.5
  |--------------|==============|==============|--------------|

  1.131          1.600          1.997          2.499          3.924


lam:

  Mean             SD               MC Error         95% HPD interval
  -------------------------------------------------------------------

  9.273            0.533            0.009            [8.269, 10.287]
  6.203            0.456            0.009            [5.349, 7.124]
  9.520            0.532            0.008            [8.479, 10.548]
  8.946            0.511            0.008            [7.854, 9.868]
  11.774           0.618            0.011            [10.608, 12.976]

  Posterior quantiles:
  2.5            25             50             75             97.5
  |--------------|==============|==============|--------------|

  8.275          8.900          9.258          9.641          10.301
  5.328          5.880          6.194          6.507          7.112
  8.527          9.157          9.517          9.860          10.612
  7.953          8.606          8.933          9.286          9.999
  10.604         11.351         11.774         12.198         12.973
</code></pre>

<p>We can also see this visually.</p>

<pre><code class="language-python">pm.plot_posterior(trace);
</code></pre>

<p><img src="intro_PyMC3_files/intro_PyMC3_46_0.png" alt="png" /></p>

<p>We can verify the convergence of the chains formally using the Gelman Rubin test. Values close to 1.0 mean convergence.</p>

<pre><code class="language-python">pm.gelman_rubin(trace)
</code></pre>

<pre><code>{'lam': array([ 0.9995478 ,  1.0001032 ,  0.99952763,  0.99959111,  1.00086874]),
 'lam_log__': array([ 0.99954011,  1.00016131,  0.99951666,  0.9996044 ,  1.00085202]),
 'mu': 1.0008168278503426,
 'mu_log__': 1.0007094063229371,
 'sigma': 1.0026185662681779,
 'sigma_log__': 1.0026433603703895}
</code></pre>

<p>We can also test for correlation between samples in the chains. We are aiming for zero auto-correlation to get &ldquo;random&rdquo; samples from the posterior distribution.</p>

<pre><code class="language-python"># Auto-correlation of the parameter sigma for the 3 chains.
pm.autocorrplot(trace, varnames=['sigma'], max_lag=20);
</code></pre>

<p><img src="intro_PyMC3_files/intro_PyMC3_50_0.png" alt="png" /></p>

<pre><code class="language-python"># We can also consider all the variables simultaneously. 
pm.autocorrplot(trace, max_lag=20);
</code></pre>

<p><img src="intro_PyMC3_files/intro_PyMC3_51_0.png" alt="png" /></p>

<p>From these plots we see that the auto-correlation is not problematic. Indeed, we can test this through the <em>effective sample size</em>, which sould be close to the total sumber of samples <code>n_sim</code>.</p>

<pre><code class="language-python">pm.diagnostics.effective_n(trace)
</code></pre>

<pre><code>{'lam': array([ 3000.,  2859.,  3000.,  3000.,  3000.]),
 'lam_log__': array([ 3000.,  2853.,  3000.,  3000.,  3000.]),
 'mu': 2715.0,
 'mu_log__': 2634.0,
 'sigma': 1081.0,
 'sigma_log__': 1089.0}
</code></pre>

<p>Finally, we can compute the <a href="https://en.wikipedia.org/wiki/Deviance_information_criterion">deviance information criterion</a>.</p>

<pre><code class="language-python">pm.dic(trace, model=model)
</code></pre>

<pre><code>795.18082904654284
</code></pre>

<h2 id="3-4-residual-analysis">3.4 Residual analysis</h2>

<p>In order to evaluate the model results we analyze the behaviour of the residuals.</p>

<pre><code class="language-python"># Compute the mean of the simulation.
lambda_mean = np.apply_along_axis(np.mean, 0, trace['lam'])

# Compute for each sample the posterior mean.
cookies['yhat'] = cookies.location.apply(lambda x: lambda_mean[x-1])

# Compute the residuals.
cookies['resid'] = cookies.apply(lambda x: x.chips - x.yhat, axis=1)

cookies.head()
</code></pre>

<div>
<style>
    .dataframe thead tr:only-child th {
        text-align: right;
    }

    .dataframe thead th {
        text-align: left;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>chips</th>
      <th>location</th>
      <th>yhat</th>
      <th>resid</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>12</td>
      <td>1</td>
      <td>9.272855</td>
      <td>2.727145</td>
    </tr>
    <tr>
      <th>1</th>
      <td>12</td>
      <td>1</td>
      <td>9.272855</td>
      <td>2.727145</td>
    </tr>
    <tr>
      <th>2</th>
      <td>6</td>
      <td>1</td>
      <td>9.272855</td>
      <td>-3.272855</td>
    </tr>
    <tr>
      <th>3</th>
      <td>13</td>
      <td>1</td>
      <td>9.272855</td>
      <td>3.727145</td>
    </tr>
    <tr>
      <th>4</th>
      <td>12</td>
      <td>1</td>
      <td>9.272855</td>
      <td>2.727145</td>
    </tr>
  </tbody>
</table>
</div>

<pre><code class="language-python"># Cookies Residuals
plt.scatter(x=cookies.index.values, y=cookies.resid.values)
plt.axhline(y=0.0, color='r', linestyle='--')
plt.title('Cookies Residuals')
plt.xlabel('Observation');
</code></pre>

<p><img src="intro_PyMC3_files/intro_PyMC3_58_0.png" alt="png" /></p>

<p>We do not see a particular partern in the scatter plot of the residuals against the observation.</p>

<pre><code class="language-python">cookies.plot.scatter(x='yhat', y='resid');
plt.axhline(y=0.0, color='r', linestyle='--')
plt.title('Cookies Residuals')
</code></pre>

<pre><code>&lt;matplotlib.text.Text at 0x120551518&gt;
</code></pre>

<p><img src="intro_PyMC3_files/intro_PyMC3_60_1.png" alt="png" /></p>

<h2 id="3-5-predictions">3.5 Predictions</h2>

<p>Finally, we are going to illustrate how to use the simulation results to derive predictions.</p>

<h3 id="3-5-1-for-a-known-location">3.5.1 For a known location</h3>

<p>Let us consider Location 1. We want, for example, to compute the posterior probability the next cookie in this location has less than 7 chips.</p>

<pre><code class="language-python"># We generate n_sim samples of a Poisson distribution for 
# each value for lam_0 (location 1) simulation..
y_pred_location_1 = np.random.poisson(lam=trace['lam'][:,0] , size=n_sim)

plt.figure()
plt.hist(y_pred_location_1, bins=20)
plt.title('Chocolate Chips Distribution for Location 1');

# Probability the next cookie in location has less than 7 chips.
(y_pred_location_1&lt;7).astype(int).mean()
</code></pre>

<pre><code>0.18533333333333332
</code></pre>

<p><img src="intro_PyMC3_files/intro_PyMC3_62_1.png" alt="png" /></p>

<h3 id="3-5-1-for-a-new-location">3.5.1 For a new location</h3>

<p>Now assume we want to open a new location. First, we want to compute the posterior probability that this new location has \(\lambda &gt; 15\).</p>

<pre><code class="language-python"># Posterior distribution of for a an b 
# from the simulated values of mu and sigma2.
post_a = trace['mu']**2/trace['sigma']**2
post_b = trace['mu']/trace['sigma']**2

# We now generate samples of a gamma distribution 
# with these generated parameters of a and b.
lambda_pred_dist = np.random.gamma(post_a,1/post_b,n_sim) 

plt.figure()
plt.hist(lambda_pred_dist, bins=40)
plt.title('Lambda Predicted Distribution');

# Posterior probability a new location has lambda &gt; 15.
(lambda_pred_dist&gt;15).astype(int).mean()
</code></pre>

<pre><code>0.017333333333333333
</code></pre>

<p><img src="intro_PyMC3_files/intro_PyMC3_64_1.png" alt="png" /></p>

<p>Now we answer a question at the next level of the hierarchical model. We want to calculate the posterior probability for a cookie produced in a new location to have more than 15 chocolate chips.</p>

<pre><code class="language-python"># Posterior distribution of the chips.
# Here we use the values of lambda obtained above.
cookies_pred_dist = np.random.poisson(lam=lambda_pred_dist, size=n_sim)

plt.figure()
plt.hist(cookies_pred_dist, bins=30)
plt.title('Chocolate Chips Distribution New Location');

# Posterior probability that a cookie produced 
# in a new location has more than 15 chocolate chips.
(cookies_pred_dist&gt;15).astype(int).mean()
</code></pre>

<pre><code>0.056333333333333332
</code></pre>

<p><img src="intro_PyMC3_files/intro_PyMC3_66_1.png" alt="png" /></p>

<h1 id="4-references-and-further-reading">4 References and Further Reading</h1>

<p>Here we provide some suggested references used in this post and also to go deeper in the subject.</p>

<h2 id="4-1-bayesian-probability">4.1 Bayesian Probability</h2>

<ul>
<li><p><a href="https://www.coursera.org/learn/bayesian-statistics">Coursera: Bayesian Statistics: From Concept to Data Analysis</a></p></li>

<li><p><a href="https://www.coursera.org/learn/mcmc-bayesian-statistics">Coursera: Bayesian Statistics: Techniques and Models</a></p></li>

<li><p><a href="http://www.springer.com/us/book/9780387922997">A First Course in Bayesian Statistical Methods, Peter D. Hoff</a></p></li>

<li><p><a href="http://www.springer.com/la/book/9780387400846">An Introduction to Bayesian Analysis: Theory and Methods, Ghosh, Jayanta K., Delampady, Mohan, Samanta, Tapas</a></p></li>
</ul>

<h2 id="4-2-pymc3">4.2 PyMC3</h2>

<ul>
<li><p><a href="https://pymc-devs.github.io/pymc3/index.html">Documentation</a></p></li>

<li><p><a href="https://arxiv.org/abs/1507.08050">Probabilistic Programming in Python using PyMC, John Salvatier, Thomas Wiecki, Christopher Fonnesbeck</a></p></li>
</ul>

    </div>
  </article>

  


</main>

      <footer class="footer">
        <ul class="footer-links">
          <li>
            <a href="/index.xml" type="application/rss+xml" target="_blank">RSS feed</a>
          </li>
          <li>
            <a href="https://gohugo.io/" class="footer-links-kudos">Made with <img src="/images/hugo-logo.png" alt="Img link to Hugo website" width="22" height="22"></a>
          </li>
        </ul>
      </footer>

    </div>
    



<script src="//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/highlight.min.js"></script>



<script src="//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/languages/r.min.js"></script>
<script src="//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/languages/yaml.min.js"></script>
<script>hljs.configure({languages: []}); hljs.initHighlightingOnLoad();</script>



    
<script src="/js/math-code.js"></script>
<script async src="//cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_CHTML"></script>


    
  </body>
</html>

