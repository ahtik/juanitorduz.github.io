<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>R on Dr. Juan Camilo Orduz</title>
    <link>/categories/r/</link>
    <description>Recent content in R on Dr. Juan Camilo Orduz</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Thu, 20 Dec 2018 00:00:00 +0000</lastBuildDate>
    
	<atom:link href="/categories/r/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>Text Mining, Networks and Visualization: Plebiscito Tweets</title>
      <link>/text-mining-networks-and-visualization-plebiscito-tweets/</link>
      <pubDate>Thu, 20 Dec 2018 00:00:00 +0000</pubDate>
      
      <guid>/text-mining-networks-and-visualization-plebiscito-tweets/</guid>
      <description>Nowadays social media generates a vast amount of raw data (text, images, videos, etc). It is a very interesting challenge to discover techniques to get insights on the content and development of social media data. In addition, as a fundamental component of the analysis, it is important to find ways of communicating the results, i.e.Â data visualization. In this post I want to present a small case study where I analyze Twitter text data.</description>
    </item>
    
    <item>
      <title>Exploring the Curse of Dimensionality</title>
      <link>/exploring-the-curse-of-dimensionality/</link>
      <pubDate>Sun, 09 Dec 2018 00:00:00 +0000</pubDate>
      
      <guid>/exploring-the-curse-of-dimensionality/</guid>
      <description>In this post I want to present the notion of curse of dimensionality following a suggested excercise (Chapter 4 - Ex. 4) of the book An Introduction to Statistical Learning, writen by Gareth James, Daniela Witten, Trevor Hastie and Robert Tibshirani.
When the number of features \(p\) is large, there tends to be a deterioration in the performance of KNN and other local approaches that perform prediction using only observations that are near the test observation for which a prediction must be made.</description>
    </item>
    
    <item>
      <title>Introduction to R Plumber : Expose a Caret model to a web API</title>
      <link>/intro_plumber/</link>
      <pubDate>Fri, 12 Oct 2018 00:00:00 +0000</pubDate>
      
      <guid>/intro_plumber/</guid>
      <description>In this post we explore the basics of the Plumber package. Our aim is to ilustrate how to fit a \(L^2\)-regularized linear model and expose it to a web API so that we can request predictions.
Prepare Notebook Let us load the necessary libraries.
library(caret) library(httr) library(magrittr) library(plumber) library(tidyverse)  Load Data As a toy example we consider the mtcars data set.
df &amp;lt;- mtcars %&amp;gt;% as_tibble() df %&amp;gt;% head  ## # A tibble: 6 x 11 ## mpg cyl disp hp drat wt qsec vs am gear carb ## &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt; ## 1 21 6 160 110 3.</description>
    </item>
    
    <item>
      <title>Circle Radius Fit for a Cloud of Points</title>
      <link>/circle-radius-fit-for-a-cloud-of-points/</link>
      <pubDate>Sun, 09 Sep 2018 00:00:00 +0000</pubDate>
      
      <guid>/circle-radius-fit-for-a-cloud-of-points/</guid>
      <description>In this post I explore how to render a .Rmd file directly with blogdown. To play around with it, I wrote a simple R notebook which fits a circle to a cloud of points.
Prepare the Notebook library(tidyverse)  Generate Circle Data # Dimension of the space. d &amp;lt;- 2 # Number of sample points. N &amp;lt;- 1000 # Radius. R &amp;lt;- 4 # Generate random sample of points (x - axis).</description>
    </item>
    
  </channel>
</rss>